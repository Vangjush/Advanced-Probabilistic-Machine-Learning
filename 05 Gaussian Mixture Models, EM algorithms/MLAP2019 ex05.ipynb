{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-0af738a02c7e>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-0af738a02c7e>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    d = x[:,np.newaxis] - centers[np.newaxis,:]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Template for exercise 4.1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the observations\n",
    "data = np.loadtxt('ex4_1_data.txt')\n",
    "x_obs = data[:,0]\n",
    "y_obs = data[:,1]\n",
    "N_train = 50\n",
    "x_train = x_obs[:N_train]\n",
    "y_train = y_obs[:N_train]\n",
    "N_test = 10\n",
    "x_test = x_obs[N_train:N_train+N_test]\n",
    "y_test = y_obs[N_train:N_train+N_test]\n",
    "x_range = (-5, 5) # Possible values of x are in this range\n",
    "# Basis function parameters\n",
    "num_basis_functions = 11\n",
    "centers = np.linspace(x_range[0], x_range[1], num_basis_functions)\n",
    "lambdaval = 0.17\n",
    "# You can use here assume the correct basis function centers and lambda ...\n",
    "def rbf(x, centers, lambdaval):\n",
    "# Radial Basis Function output for input x\n",
    "#\n",
    "# Inputs:\n",
    "# x : input points (one-dimensional array)\n",
    "# centers : basis function centers (one-dimensional array)\n",
    "# lambdaval : basis function width (scalar)\n",
    "#\n",
    "# Output:\n",
    "# Radial Basis Functions evaluated at x (two-dimensional array with len(x)\n",
    "# rows and len(centers) columns)\n",
    "d = x[:,np.newaxis] - centers[np.newaxis,:]\n",
    "y = np.exp(-0.5 * (d ** 2) / lambdaval)\n",
    "return y\n",
    "def bayesian_linear_regression(phi_x, y, alpha, beta):\n",
    "# Bayesian linear parameter model\n",
    "#\n",
    "# Inputs:\n",
    "# phi_x : the basis function applied to x-data (two-dimensional array)\n",
    "# y : y-data (one-dimensional array)\n",
    "# alpha : the precision of the weight prior distribution (scalar)\n",
    "# beta : the precision of the assumed gaussian noise (scalar)\n",
    "#\n",
    "# Output:\n",
    "# the posterior mean, the posterior covariance, the log marginal likelihood\n",
    "N, B = phi_x.shape\n",
    "# Add here code to compute:\n",
    "# m = the posterior mean of w\n",
    "# S = the posterior covariance of w\n",
    "# S_inv = the inverse of S\n",
    "# Equation\n",
    "S_inv = (alpha * np.identity(B) + beta * np.dot(phi_x.T,phi_x))\n",
    "S = np.linalg.inv(S_inv)\n",
    "# Note: This is a corrected version of equation 18.1.19 from Barbers book\n",
    "d = beta * np.dot(phi_x.T, y)\n",
    "m = beta * S @ phi_x.T @ y\n",
    "log_likelihood = 0.5 * (-beta * np.dot(y, y) + d @ S @ d + np.log(np.linalg.det(2 *\n",
    "np.pi * S)) + B * np.log(alpha) + N * np.log(beta) - N * np.log(2 * np.pi))\n",
    "return m, S, log_likelihood\n",
    "# Problem 1\n",
    "# Specify possible values for the alpha and beta parameters to test\n",
    "alphas = np.logspace(-3, 3, 100)\n",
    "betas = np.logspace(-3, 3, 100)\n",
    "ll_best = 0\n",
    "# Grid search over possible values of alpha and beta\n",
    "for alpha in alphas:\n",
    "for beta in betas:\n",
    "# Use here functions rbf and bayesian_linear_regression to compute the\n",
    "# log marginal likelihood for given alpha and beta\n",
    "phi_x = rbf(x_train, centers, lambdaval)\n",
    "_, __, log_likelihood = bayesian_linear_regression(phi_x, y_train, alpha, beta)\n",
    "# What are the optimal values of alpha and beta, that maximize the marginal\n",
    "# likelihood?\n",
    "if log_likelihood > ll_best:\n",
    "ll_best = log_likelihood\n",
    "best_alpha = alpha\n",
    "best_beta = beta\n",
    "# Fit the model one more time using the optimal alpha and beta and the\n",
    "training\n",
    "# data to get m for the optimal model\n",
    "print('Best alpha :', best_alpha)\n",
    "print('Best beta :', best_beta)\n",
    "best_m, _ , __ = bayesian_linear_regression(phi_x, y_train, best_alpha, best_beta)\n",
    "# Compute the final regression function\n",
    "x_coord = np.linspace(x_range[0], x_range[1], 100)\n",
    "# Compute the predicted values for inputs in x_coord using best_m\n",
    "y_mean = rbf(x_coord, centers, lambdaval) @ best_m.T\n",
    "# Plot the final learned regression function, together with the samples\n",
    "plt.plot(x_coord, y_mean, label=\"learned model\")\n",
    "plt.plot(x_train, y_train, 'kx', label=\"training data\")\n",
    "plt.plot(x_test, y_test, 'rx', label=\"testing data\")\n",
    "# Make predictions for inputs in the test data, so that you get\n",
    "# predictions 'y_pred' for inputs in x_test.\n",
    "y_pred = rbf(x_test, centers, lambdaval) @ best_m.T ##WHY???????\n",
    "# Plot the predictions\n",
    "plt.plot(x_test, y_pred, 'gx', label=\"testing predictions\")\n",
    "# Compute the mean squared prediction error for the test data.\n",
    "mse_test = 1 / N_test * np.sum(y_test @ y_test - 2*y_test@y_pred +\n",
    "y_pred@y_pred)\n",
    "plt.legend()\n",
    "plt.title(\"ML-II: $\\\\alpha$=%.3f, $\\\\beta$=%.3f, mse=%.4f\" % (best_alpha,\n",
    "best_beta, mse_test))\n",
    "plt.show()\n",
    "################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
